{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data and do EDA(Exploratory Data Analysis)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf_train = pd.read_csv('/kaggle/input/shopee-workshop-building-ann/HR-Employee-Attrition_train.csv')\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src= 'https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4342316%2F9c2161bc579fae87686285d953bdf56d%2Fdata%20description.png?generation=1591008771269892&alt=media'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if there is any missing value\ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check numerical data columns\nprint('There are %s numerical columns.'%(str(len(df_train.select_dtypes(include=['int64']).columns))))\ndf_train.select_dtypes(include=['int64']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check categorical data columns \nprint('There are %s numerical columns.'%(str(len(df_train.select_dtypes(include=['object']).columns))))\ndf_train.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we can divide all these 35 columns into 4 parts:\n1. id column\n2. target column\n3. numerical columns\n4. categorical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"id_col = 'EmployeeNumber'\n\ntarget_col = 'Attrition'\n\n#25 numerical columns\nnumerical_cols = ['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EmployeeCount', 'EnvironmentSatisfaction', \n                'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', \n                'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', \n                'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', \n                'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n\n#8 categorical columns\ncategorical_cols = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', \n                 'Over18', 'OverTime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check 8 categorical columns\nnunique_values = []\nna_values = []\nfor col in categorical_cols:\n    nunique_values.append(df_train[col].nunique())\n    na_values.append(len(df_train.loc[df_train[col].isna()]))\ndf_train_stats_categorical = pd.DataFrame(list(zip(categorical_cols,nunique_values,na_values)),columns=['Column_Name','#Unique_Values','#Null_Value'])\ndf_train_stats_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check Null columns\ndf_train_stats_categorical[df_train_stats_categorical['#Null_Value']>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check columns with only 1 value\ndf_train_stats_categorical[df_train_stats_categorical['#Unique_Values'] == 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column 'Over18' only contains 1 unique value, therefore we could drop it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check 25 numerical columns\nna_values = []\nfor col in numerical_cols:\n    na_values.append(len(df_train.loc[df_train[col].isna()]))\ndf_train_stats_numerical = pd.DataFrame(list(zip(numerical_cols,na_values)),columns=['Column_Name','#Null_Value'])\ndf_train_stats_numerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check Null columns\ndf_train_stats_numerical[df_train_stats_numerical['#Null_Value']>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_stats_numerical = df_train[numerical_cols].describe().T\ndf_train_stats_numerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check columns with only 1 value\ndf_train_stats_numerical[df_train_stats_numerical['std']==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns like 'EmployeeCount' and 'StandardHours' have 0 std, meaning all the values are the same, therefore we can drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check categorical columns\ndef explore_categorical_columns(df,categorical_cols):\n    nunique_values = []\n    na_values = []\n    for col in categorical_cols:\n        nunique_values.append(df[col].nunique())\n        na_values.append(len(df.loc[df[col].isna()]))\n    df_stats_categorical = pd.DataFrame(list(zip(categorical_cols,nunique_values,na_values)),columns=['Column_Name','#Unique_Values','#Null_Value'])\n    df_null = df_stats_categorical[df_stats_categorical['#Null_Value']>0]\n    df_unique_value = df_stats_categorical[df_stats_categorical['#Unique_Values'] == 1]\n    if len(df_null) > 0:\n        print('Columns with Null value: %s'%(str(df_null['Column_Name'])))\n    else:\n        print('There is no Null values in the categorical columns')\n    if len(df_unique_value) > 0:\n        print('Columns with only 1 unique value: %s'%(str(list(df_unique_value['Column_Name']))))\n    else:\n        print('All categorical columns have more than 1 value.')\n        \n#check numerical columns\ndef explore_numerical_columns(df,numerical_cols):\n    na_values = []\n    for col in numerical_cols:\n        na_values.append(len(df.loc[df[col].isna()]))\n    df_stats_numerical = pd.DataFrame(list(zip(numerical_cols,na_values)),columns=['Column_Name','#Null_Value'])\n    df_null = df_stats_numerical[df_stats_numerical['#Null_Value']>0]\n    df_stats = df[numerical_cols].describe().T\n    df_unique_value = df_stats[df_stats['std']==0]\n    \n    if len(df_null) > 0:\n        print('Columns with Null value: %s'%(str(df_null['Column_Name'])))\n    else:\n        print('There is no Null values in the numerical columns')\n    \n    if len(df_unique_value) > 0:\n        print('Columns with only 1 unique value: %s'%(str(list(df_unique_value.index))))\n    else:\n        print('All numerical columns have more than 1 value. \\n')\n\n#check categorical columns\nprint('Categorical_Columns'.center(50,\"*\"))\nexplore_categorical_columns(df_train,categorical_cols)\nprint('Numerical_Columns'.center(50,\"*\"))\n#check numerical columns\nexplore_numerical_columns(df_train,numerical_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The column 'Over18' only contains 1 unique value, therefore we could drop it.\n- Columns like 'EmployeeCount' and 'StandardHours' have 0 std, meaning all the values are the same, therefore we can drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check target variable\ndf_train[target_col].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Process","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"    # Scaling Numerical Data\n<img src=\"https://miro.medium.com/max/1276/0*_apuT0HdrVYMUCh7\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\ndef process_numerical_data(df,numerical_cols,drop_numerical_columns):\n    sacalar = MinMaxScaler()\n    # sacalar = StandardScaler() if using StandardScaler\n    scale_numerical_cols = list(set(numerical_cols)-set(drop_numerical_columns))\n    df_numerical = sacalar.fit_transform(df[scale_numerical_cols])\n    df_numerical = pd.DataFrame(df_numerical,columns=scale_numerical_cols)\n    return df_numerical\n\ndrop_numerical_columns = ['EmployeeCount', 'StandardHours']\ndf_train_numerical = process_numerical_data(df_train,numerical_cols,drop_numerical_columns)\ndf_train_numerical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    # Convert categorical columns into numerical columns\n<img src=\"https://i.imgur.com/mtimFxh.png\">\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[categorical_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?pd.get_dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_categorical_data(df,categorical_cols,drop_categorical_columns):\n    df_categorical = pd.get_dummies(df[[var for var in categorical_cols if var not in drop_categorical_columns]])\n    return df_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_categorical_columns = ['Over18']\ndf_train_categorical = process_categorical_data(df_train, categorical_cols, drop_categorical_columns)\ndf_train_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_numerical.shape, df_train_categorical.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_features = pd.concat([df_train_numerical, df_train_categorical],axis=1)\ndf_train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_label(df):\n    target_col_dict = {'Yes': 1, 'No': 0}\n    df_labels = df[target_col].map(target_col_dict).values\n    return df_labels\ndf_train_labels = process_label(df_train)\ndf_train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into train and test\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(df_train_features,df_train_labels,test_size=0.3,random_state=23)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set up and train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nNN = MLPClassifier()\nNN.fit(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_predicted_y = NN.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(test_y,NN_predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the test dataset and do the EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/shopee-workshop-building-ann/HR-Employee-Attrition_test.csv')\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Missing columns in test dataset(campared with train dataset): %s'%(str(set(df_train.columns)-set(df_test.columns))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can continue using the same 4 group column types:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"id_col = 'EmployeeNumber'\n\nnumerical_cols = ['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EmployeeCount', 'EnvironmentSatisfaction', 'HourlyRate', \n                  'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', \n                  'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', \n                  'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', \n                  'YearsSinceLastPromotion', 'YearsWithCurrManager']\n\ncategorical_cols = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check 8 categorical columns\nprint('Categorical_Columns'.center(50,\"*\"))\nexplore_categorical_columns(df_test,categorical_cols)\nprint('Numerical_Columns'.center(50,\"*\"))\nexplore_numerical_columns(df_test,numerical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_numerical_columns = ['EmployeeCount', 'StandardHours']\ndf_test_numerical = process_numerical_data(df_test,numerical_cols,drop_numerical_columns)\ndf_test_numerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to check if there's any missing numerical columns in test dataset\nset(df_train_numerical.columns)-set(df_test_numerical.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_categorical_columns = ['Over18']\ndf_test_categorical = process_categorical_data(df_test,categorical_cols,drop_categorical_columns)\ndf_test_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to check if there's any missing categorical columns in test dataset\nset(df_train_categorical.columns)-set(df_test_categorical.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_features = pd.concat([df_test_numerical, df_test_categorical],axis=1)\ndf_test_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make prediction on test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_test = NN.predict(df_test_features)\npredicted_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer = df_test[['EmployeeNumber']]\nanswer['Attrition'] = ['Yes' if i == 1 else 'No' for i in predicted_test]\nanswer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#You can use below code to download the answer:\n\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# GridSearch to find the optimal parameter setting\nMost used parameter\n\n1）The ith element represents the number of neurons in the ith hidden layer.\n    example: ``hidden_layer_sizes``=(10)\n    \n2）activation function\n    example: ``activation``=\"relu\"\n\n3）The solver for weight optimization. {'lbfgs', 'sgd', 'adam'}\n    example: ``solver``='adam'\n\n4）L2 penalty (regularization term) parameter\n    example: ``alpha``=0.0001\n\n5）Size of minibatches for stochastic optimizers.\n    If the solver is 'lbfgs', the classifier will not use minibatch.\n    When set to 'auto', 'batch_size=min(200, n_samples)'\n    example: ``batch_size``='auto'\n\n6）Learning rate schedule for weight updates.\n    example: ``learning_rate``=\"constant\"\n\n7）The initial learning rate used. It controls the step-size in updating the weights. Only used when solver='sgd' or 'adam'.\n    example: ``learning_rate_init``=0.001\n\n8）Maximum number of iterations. The solver iterates until convergence\n    example: ``max_iter``=200\n\n9）Tolerance for the optimization. When the loss or score is not improving\n    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n    unless ``learning_rate`` is set to 'adaptive', convergence is\n    considered to be reached and training stops.\n    example: ``tol``=1e-4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport time\nstart_time = time.time()\nNN_new = MLPClassifier(max_iter=1000)\nparameter_space = {\n    'hidden_layer_sizes': [(50,50,50), (50,100,50), (50,100,50,25)],\n    'activation': ['tanh', 'relu','logistic'],\n    'solver': ['adam', 'lbfgs'],\n    'alpha': [0.001, 0.01],\n    'learning_rate': ['constant','adaptive'],\n}\nfrom sklearn.model_selection import GridSearchCV\n\nclf = GridSearchCV(NN_new, parameter_space, n_jobs=-1, cv=10)\nclf.fit(train_x, train_y)\nprint('Time taken for training the model: '+ str(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best paramete set\nprint('Best parameters found:\\n', clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All results\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true, y_pred = test_y , clf.predict(test_x)\nfrom sklearn.metrics import classification_report\nprint('Results on the test set:')\nprint(classification_report(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y_pred = clf.predict(df_test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = pd.DataFrame.from_dict(dict({'EmployeeNumber':list(df_test['EmployeeNumber']),\n                                         'Attrition':['Yes' if i == 1 else 'No' for i in test_y_pred]}))\ndf_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.merge(df_result,answer,on='EmployeeNumber',how='inner')\ntemp.loc[~(temp['Attrition_x']==temp['Attrition_y'])]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}