{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in c:\\programdata\\anaconda3\\lib\\site-packages (0.42.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba.posseg as pseg\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.42.1'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('shopee-product-title-translation-open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = pd.read_csv('train_en.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train_tcn = pd.read_csv('train_tcn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_style": "split",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gucci Gucci Guilty Pour Femme Stud Edition 罪愛女...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>（二手）PS4 GTA 5 俠盜獵車手5 Grand Theif Auto V繁體 中文版</td>\n",
       "      <td>Game Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>百獸卡</td>\n",
       "      <td>Life &amp; Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nac nac活氧全效柔衣素</td>\n",
       "      <td>Mother &amp; Baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Nike耐吉官方F.C. 男子足球長褲新款標準型 拒水 拉鏈褲腳\\nCD0557</td>\n",
       "      <td>Men's Apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>Dress</td>\n",
       "      <td>Women's Apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>Lilian Lin</td>\n",
       "      <td>Food &amp; Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>77 抹茶杏仁乳加 77乳加 減甜 大人味 大人的77 宇治抹茶 杏仁 宇治抹茶杏仁 抹茶 ...</td>\n",
       "      <td>Food &amp; Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>Panasonic 國際牌  電動 牙刷頭  (EW-DM81 專用刷頭) WEW0974-W</td>\n",
       "      <td>Home Electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>正品 新款紅眼 戰神哈奴曼手鐲 哈魯曼手環 戰神哈努曼 龍波雨水 新款紅眼 提升勇氣自信心 ...</td>\n",
       "      <td>Life &amp; Entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            product_title  \\\n",
       "0       Gucci Gucci Guilty Pour Femme Stud Edition 罪愛女...   \n",
       "1           （二手）PS4 GTA 5 俠盜獵車手5 Grand Theif Auto V繁體 中文版   \n",
       "2                                                     百獸卡   \n",
       "3                                          nac nac活氧全效柔衣素   \n",
       "4               #Nike耐吉官方F.C. 男子足球長褲新款標準型 拒水 拉鏈褲腳\\nCD0557   \n",
       "...                                                   ...   \n",
       "499995                                              Dress   \n",
       "499996                                         Lilian Lin   \n",
       "499997  77 抹茶杏仁乳加 77乳加 減甜 大人味 大人的77 宇治抹茶 杏仁 宇治抹茶杏仁 抹茶 ...   \n",
       "499998    Panasonic 國際牌  電動 牙刷頭  (EW-DM81 專用刷頭) WEW0974-W   \n",
       "499999  正品 新款紅眼 戰神哈奴曼手鐲 哈魯曼手環 戰神哈努曼 龍波雨水 新款紅眼 提升勇氣自信心 ...   \n",
       "\n",
       "                    category  \n",
       "0            Health & Beauty  \n",
       "1               Game Kingdom  \n",
       "2       Life & Entertainment  \n",
       "3              Mother & Baby  \n",
       "4              Men's Apparel  \n",
       "...                      ...  \n",
       "499995       Women's Apparel  \n",
       "499996      Food & Beverages  \n",
       "499997      Food & Beverages  \n",
       "499998       Home Electronic  \n",
       "499999  Life & Entertainment  \n",
       "\n",
       "[500000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cell_style": "split",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recollections Color Splash Clear Stamps &amp; Stencil</td>\n",
       "      <td>Hobbies &amp; Stationery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soap,lotion scrub set 400</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spigen Galaxy S10e Case Tough Armor Gunmetal</td>\n",
       "      <td>Mobile Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acrylic Lanalon Bright Red</td>\n",
       "      <td>Hobbies &amp; Stationery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>303 FLAT SHEET/Blanket 100% cotton</td>\n",
       "      <td>Home &amp; Living</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>rocker arm roller racing mio</td>\n",
       "      <td>Motors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>Secosana (preloved bag)</td>\n",
       "      <td>Women's Bags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>jag bag</td>\n",
       "      <td>Women's Bags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>Baby wipes 15 sheets (Alcohol and Paraben Free...</td>\n",
       "      <td>Babies &amp; Kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>PRE-LOVED ORIGINAL GREEN FINO BAG</td>\n",
       "      <td>Women's Bags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            product_title  \\\n",
       "0       Recollections Color Splash Clear Stamps & Stencil   \n",
       "1                               soap,lotion scrub set 400   \n",
       "2            Spigen Galaxy S10e Case Tough Armor Gunmetal   \n",
       "3                              Acrylic Lanalon Bright Red   \n",
       "4                      303 FLAT SHEET/Blanket 100% cotton   \n",
       "...                                                   ...   \n",
       "499995                       rocker arm roller racing mio   \n",
       "499996                            Secosana (preloved bag)   \n",
       "499997                                            jag bag   \n",
       "499998  Baby wipes 15 sheets (Alcohol and Paraben Free...   \n",
       "499999                  PRE-LOVED ORIGINAL GREEN FINO BAG   \n",
       "\n",
       "                      category  \n",
       "0         Hobbies & Stationery  \n",
       "1       Health & Personal Care  \n",
       "2           Mobile Accessories  \n",
       "3         Hobbies & Stationery  \n",
       "4                Home & Living  \n",
       "...                        ...  \n",
       "499995                  Motors  \n",
       "499996            Women's Bags  \n",
       "499997            Women's Bags  \n",
       "499998           Babies & Kids  \n",
       "499999            Women's Bags  \n",
       "\n",
       "[500000 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "https://github.com/crownpku/Awesome-Chinese-NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Jieba\n",
    "https://github.com/fxsjy/jieba\n",
    "\n",
    "Support three types of segmentation mode:\n",
    "- Accurate Mode attempts to cut the sentence into the most accurate segmentations, which is suitable for text analysis.\n",
    "- Full Mode gets all the possible words from the sentence. Fast but not accurate.\n",
    "- Search Engine Mode, based on the Accurate Mode, attempts to cut long words into several short words, which can raise the recall rate. Suitable for search engines.\n",
    "\n",
    "**How to apply this in shopee challenge?**\n",
    "- adding custom words in the dictionary\n",
    "    - need to find updated libraries\n",
    "    - subwords\n",
    "    - maybe we can get most common words in english, translate it to chinese to add as custom words\n",
    "- using the jieba.cut_for_search to tokenize the statement\n",
    "\n",
    "https://medium.com/@jjsham/nlp-tokenizing-chinese-phases-3302da4336bf\n",
    "- The major difference between Chinese and English is that structure of writing. The problem of NLP in Chinese is: If you tokenize Chinese characters from the articles, there is no whitespace in between phrases in Chinese so simply split based on whitespace like in English may not work as well as in English. What I mean is it could work, but very likely not. I will explain what it could work and what to do if it does not work. Note that I will do the experiment in traditional Chinese characters to avoid the problem of simplified Chinese.\n",
    "- Jieba is a good package to use for preprocessing but it is best not to use this package to preprocess written Cantonese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut\n",
    "- The jieba.cut function accepts three input parameters: the first parameter is the string to be cut; the second parameter is cut_all, controlling the cut mode; the third parameter is to control whether to use the Hidden Markov Model.\n",
    "- **jieba.cut_for_search accepts two parameter: the string to be cut; whether to use the Hidden Markov Model. This will cut the sentence into short words suitable for search engines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Gucci,  , Gucci,  , Guilty,  , Pour,  , Femme...\n",
       "1    [（, 二手, ）, PS4,  , GTA,  , 5,  , 俠盜, 獵車, 手, 5,...\n",
       "2                                              [百獸, 卡]\n",
       "3                           [nac,  , nac, 活氧全, 效柔, 衣素]\n",
       "4    [#, Nike, 耐吉, 官方, F, ., C, .,  , 男子, 足球, 長, 褲,...\n",
       "Name: product_title, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tcn.head().product_title.apply(lambda x: list(jieba.cut_for_search(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.lcut and jieba.lcut_for_search returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Gucci,  , Gucci,  , Guilty,  , Pour,  , Femme...\n",
       "1    [（, 二手, ）, PS4,  , GTA,  , 5,  , 俠盜, 獵車, 手, 5,...\n",
       "2                                              [百獸, 卡]\n",
       "3                           [nac,  , nac, 活氧全, 效柔, 衣素]\n",
       "4    [#, Nike, 耐吉, 官方, F, ., C, .,  , 男子, 足球, 長, 褲,...\n",
       "Name: product_title, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tcn.head().product_title.apply(jieba.lcut_for_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.Tokenizer(dictionary=DEFAULT_DICT) \n",
    "- creates a new customized Tokenizer, which enables you to use different dictionaries at the same time. jieba.dt is the default Tokenizer, to which almost all global functions are mapped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dictionary\n",
    "- The dictionary format is the same as that of dict.txt: one word per line; each line is divided into three parts separated by a space: word, word frequency, POS tag. If file_name is a path or a file opened in binary mode, the dictionary must be UTF-8 encoded.\n",
    "- The word frequency and POS tag can be omitted respectively. The word frequency will be filled with a suitable value if omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify dictionary\n",
    "\n",
    "- Use add_word(word, freq=None, tag=None) and del_word(word) to modify the dictionary dynamically in programs.\n",
    "- Use suggest_freq(segment, tune=True) to adjust the frequency of a single word so that it can (or cannot) be segmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.add_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.suggest_freq(('中', '将'), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Extraction\n",
    "\n",
    "- sentence: the text to be extracted\n",
    "- topK: return how many keywords with the highest TF/IDF weights. The default value is 20\n",
    "- withWeight: whether return TF/IDF weights with the keywords. The default value is False\n",
    "- allowPOS: filter words with which POSs are included. Empty for no filtering.\n",
    "\n",
    "**not sure if we will need this with shopee since we need as much words as possible in the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developers can specify their own custom IDF corpus in jieba keyword extraction\n",
    "- Custom Corpus Sample：https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big\n",
    "\n",
    "Developers can specify their own custom stop words corpus in jieba keyword extraction\n",
    "- Custom Corpus Sample：https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.analyse.set_idf_path(file_name)\n",
    "jieba.analyse.set_stop_words(file_name) # file_name is the path for the custom corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "\n",
    "Tags the POS of each word after segmentation, using labels compatible with ictclas.\n",
    "\n",
    "jieba.posseg.POSTokenizer(tokenizer=None) creates a new customized Tokenizer. tokenizer specifies the jieba.Tokenizer to internally use. jieba.posseg.dt is the default POSTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Processing\n",
    "\n",
    "Principle: Split target text by line, assign the lines into multiple Python processes, and then merge the results, which is considerably faster.\n",
    "\n",
    "Note that parallel processing supports only default tokenizers, jieba.dt and jieba.posseg.dt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "jieba: parallel mode only supports posix system",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-168476896417>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36menable_parallel\u001b[1;34m(processnum)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m         raise NotImplementedError(\n\u001b[1;32m--> 602\u001b[1;33m             \"jieba: parallel mode only supports posix system\")\n\u001b[0m\u001b[0;32m    603\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: jieba: parallel mode only supports posix system"
     ]
    }
   ],
   "source": [
    "jieba.enable_parallel(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, doesnt support windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "- The input must be unicode\n",
    "\n",
    "#### Default mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "?jieba.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'永和服装饰品有限公司')\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search mode - for finer segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限\t\t start: 6 \t\t end:8\n",
      "word 公司\t\t start: 8 \t\t end:10\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'永和服装饰品有限公司', mode='search')\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChineseAnalyzer for Whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChineseAnalyzer' from 'jieba.analyse' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jieba\\analyse\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-c7e07fde58e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChineseAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ChineseAnalyzer' from 'jieba.analyse' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jieba\\analyse\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from jieba.analyse import ChineseAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries to Use\n",
    "\n",
    "It is possible to use your own dictionary with Jieba, and there are also two dictionaries ready for download:\n",
    "\n",
    "1. A smaller dictionary for a smaller memory footprint: https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small\n",
    "\n",
    "2. **IMPT: There is also a bigger dictionary that has better support for traditional Chinese (繁體): https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.set_dictionary(\"D:\\Shopee\\Open Round 4_Title Translation\\dict.txt.big.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorials\n",
    "\n",
    "1. How to improve the performance of chinese text tokenization in python and jieba\n",
    "https://levelup.gitconnected.com/how-to-improve-the-performance-of-chinese-text-tokenization-in-python-and-jieba-26add53f3756\n",
    "    - adding custom words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\rdico\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.885 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# x\n",
      "Nike eng\n",
      "耐吉 nz\n",
      "官方 n\n",
      "F eng\n",
      ". m\n",
      "C eng\n",
      ". m\n",
      "  x\n",
      "男子 n\n",
      "足球 n\n",
      "長 a\n",
      "褲 n\n",
      "新款 n\n",
      "標準 n\n",
      "型 k\n",
      "  x\n",
      "拒水 v\n",
      "  x\n",
      "拉 v\n",
      "鏈 ng\n",
      "褲 ng\n",
      "腳 n\n",
      "\\ x\n",
      "nCD0557 eng\n"
     ]
    }
   ],
   "source": [
    "text = train_tcn.iloc[4,0]\n",
    "words = pseg.cut(text)\n",
    "for w in words:\n",
    "    print('%s %s' % (w.word, w.flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add custom words\n",
    "add_word(word, freq=None, tag=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.add_word('于吉', freq=None, tag='nr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample code for adding nouns\n",
    "with open('noun_list.txt', 'r', encoding='utf8') as f:\n",
    "    custom_noun = f.readlines()\n",
    "    for noun in custom_noun:\n",
    "        jieba.add_word(noun.replace('\\n', ''), freq=None, tag='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Adding spaces\n",
    "\n",
    "https://breezegeography.wordpress.com/2018/01/25/how-to-segment-chinese-texts-putting-in-spaces-with-jieba/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command tells Jieba to put a space between 非 and 农业. Alternatively, if Jieba defaulted to separating the characters and I wanted them to be considered one unit, I’d alter the code inside the parentheses to read ‘非农业’ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq(('非','农业'), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line triggers the actual segmentation process. Jieba has a couple segmentation modes, by setting “cut_all = False” I indicate that I want Jieba to run its slower, more accurate mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Nike',\n",
       " '耐吉',\n",
       " '官方',\n",
       " 'F',\n",
       " '.',\n",
       " 'C',\n",
       " '.',\n",
       " ' ',\n",
       " '男子',\n",
       " '足球',\n",
       " '長',\n",
       " '褲',\n",
       " '新款',\n",
       " '標準',\n",
       " '型',\n",
       " ' ',\n",
       " '拒水',\n",
       " ' ',\n",
       " '拉鏈',\n",
       " '褲腳',\n",
       " '\\\\',\n",
       " 'nCD0557']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = jieba.cut(text, cut_all=False)\n",
    "list(words)\n",
    "# for w in words:\n",
    "#     print('%s %s' % (w.word, w.flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing: hanlp\n",
    "\n",
    "The multilingual NLP library for researchers and companies, built on TensorFlow 2.0, for advancing state-of-the-art deep learning techniques in both academia and industry.\n",
    "\n",
    "https://github.com/hankcs/HanLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "HanLP will automatically resolve the identifier CTB6_CONVSEG to an URL, then download it and unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hanlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a80452a53dc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhanlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhanlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CTB6_CONVSEG'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hanlp'"
     ]
    }
   ],
   "source": [
    "import hanlp\n",
    "tokenizer = hanlp.load('CTB6_CONVSEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('商品和服务')\n",
    "['商品', '和', '服务']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can predict much faster. In the era of deep learning, batched computation usually gives a linear scale-up factor of batch_size. So, you can predict multiple sentences at once, at the cost of GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hanlp.pretrained.ALL #list all pre-trained models\n",
    "hanlp.pretrained.* #browse pre-trained models by categories of NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "Did you notice the different pos tags for the same word 希望 (\"hope\")? The first one means \"my dream\" as a noun while the later means \"want\" as a verb. This tagger uses fasttext[^fasttext] as its embedding layer, which is free from OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)\n",
    "tagger(['我', '的', '希望', '是', '希望', '和平'])\n",
    "['PN', 'DEG', 'NN', 'VC', 'VV', 'NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "The NER component requires tokenized tokens as input, then outputs the entities along with their types and spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)\n",
    "recognizer([list('上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。'),\n",
    "                list('萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。')])\n",
    "[[('上海华安工业（集团）公司', 'NT', 0, 12), ('谭旭光', 'NR', 15, 18), ('张晚霞', 'NR', 21, 24), ('美国', 'NS', 26, 28), ('纽约现代艺术博物馆', 'NS', 28, 37)], \n",
    " [('萨哈夫', 'NR', 0, 3), ('伊拉克', 'NS', 5, 8), ('联合国销毁伊拉克大规模杀伤性武器特别委员会', 'NT', 10, 31)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Since parsers require part-of-speech tagging and tokenization, while taggers expects tokenization to be done beforehand, wouldn't it be nice if we have a pipeline to connect the inputs and outputs, like a computation graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = hanlp.pipeline() \\\n",
    "    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\\n",
    "    .append(tokenizer, output_key='tokens') \\\n",
    "    .append(tagger, output_key='part_of_speech_tags') \\\n",
    "    .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies') \\\n",
    "    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline(text)) #output is json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save('zh.json') #save output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train own models\n",
    "\n",
    "To write DL models is not hard, the real hard thing is to write a model able to reproduce the score in papers. The snippet below shows how to train a 97% F1 cws model on MSR corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NgramConvTokenizer()\n",
    "save_dir = 'data/model/cws/convseg-msr-nocrf-noembed'\n",
    "tokenizer.fit(SIGHAN2005_MSR_TRAIN,\n",
    "              SIGHAN2005_MSR_VALID,\n",
    "              save_dir,\n",
    "              word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n",
    "                          'config': {\n",
    "                              'trainable': True,\n",
    "                              'filepath': CONVSEG_W2V_NEWS_TENSITE_CHAR,\n",
    "                              'expand_vocab': False,\n",
    "                              'lowercase': False,\n",
    "                          }},\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                 epsilon=1e-8, clipnorm=5),\n",
    "              epochs=100,\n",
    "              window_size=0,\n",
    "              metrics='f1',\n",
    "              weight_norm=True)\n",
    "tokenizer.evaluate(SIGHAN2005_MSR_TEST, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO READ\n",
    "\n",
    "- https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3\n",
    "- https://github.com/google/sentencepiece\n",
    "- https://pypi.org/project/hanlp/#description\n",
    "- https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46#:~:text=Introduction%20to%20subword&text=Subword%20is%20in%20between%20word,)%20to%20represent%20%E2%80%9Csubword%E2%80%9D.\n",
    "- https://github.com/crownpku/Awesome-Chinese-NLP\n",
    "- https://towardsdatascience.com/beginners-guide-to-sentiment-analysis-for-simplified-chinese-using-snownlp-ce88a8407efb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235.837px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
